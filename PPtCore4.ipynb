{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3829b6-f7e5-4da5-8895-17e81a416ea4",
   "metadata": {},
   "source": [
    "1. What is the purpose of the General Linear Model (GLM)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027a84e-7c35-495e-98e5-5eacfa690b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The General Linear Model (GLM) is a useful framework for comparing how several variables affect different\n",
    "    continuous variables1. GLM is the foundation for several statistical tests, including ANOVA, ANCOVA and \n",
    "    regression analysis1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a415f25-b1fc-47b6-9466-bebe280483ed",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of the General Linear Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab3f4f-63d3-4e1f-afa7-cf0dd8e83a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The general linear model makes four assumptions:\n",
    "    Linearity: The relationship between the predictors and the outcome is linear.\n",
    "    Homoscedasticity: The variance of the errors is constant for any value of the predictors.\n",
    "    Normality: The errors are normally distributed for any value of the predictors.\n",
    "    Independence: The errors are independent of each other.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8d12c-d478-4098-9369-7b06c7ad784e",
   "metadata": {},
   "source": [
    "3. How do you interpret the coefficients in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2404c-3a2d-4707-884d-788a17a034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The GLM coefficients only show the multiplicative change in odds ratio. so if p1 is the risk of getting a high\n",
    "score for black defendants and p0 is the risk of getting a high score for white defendants, then exp(0.47721) \n",
    "shows (p1/(1-p1))/(p0/(1-p0)). Unfortunately, this is not particularly easy to intuit. Relative risk (p1/p0) is a\n",
    "much simpler concept to understand and odds ratio can be converted to relative risk using the formula: Relative \n",
    "risk=odds ratio/(1−p0+(p0×odds ratio)). The calculation in the question accomplishes that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e0c94-1602-4933-888e-9cf14081daf6",
   "metadata": {},
   "source": [
    "4. What is the difference between a univariate and multivariate GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5950e2-3983-4e22-b6b2-66193f53be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are\n",
    "     tested independently, i.e., as multiple univariate tests with the same design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fbdd3-3eea-48d8-9c0a-d69ac1fb5d4d",
   "metadata": {},
   "source": [
    "5. Explain the concept of interaction effects in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdda9e-de09-4eb8-8744-342b846b7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The interaction effect is the difference in main effect with other categories on other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c23d1-06d6-4c38-8968-7ce05a7a6252",
   "metadata": {},
   "source": [
    "6. How do you handle categorical predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00568804-8bba-412d-b381-03c4cd0f743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans If you have a dataset array containing your data, GeneralizedLinearModel.fit will automatically recognize \n",
    "    any predictor variables that are categorical and do the right thing. By categorical, I mean that you have\n",
    "    already converted the predictor to be nominal or ordinal. There's an example of using categorical predictors \n",
    "    in the LinearModel documentation, it's the exact same idea for GeneralizedLinearModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd5ff5-73a7-45df-845a-e95c776d43e4",
   "metadata": {},
   "source": [
    "7. What is the purpose of the design matrix in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cf6e5-111e-4e75-a109-4ad9e26b5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Using the General Linear Model (GLM), the statistical model specified in a design matrix is compared with the \n",
    "     measured time course at each voxel. The comparison of the model and the data is expressed as an R or F value \n",
    "     for each voxel which tells how good the overall model fits or explains the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00998f70-7bb7-4612-b641-a5562829e4f8",
   "metadata": {},
   "source": [
    "8. How do you test the significance of predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa756c5-b0fb-460a-938e-634917ebd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  You can either do an asymptotic chi-square test of (59.598-50.611) vs a chi-square with (58-56) df, or use \n",
    "     anova () on your glm object (that doesn't do the test directly, but at least calculates (59.598-50.611) and \n",
    "    (58-56) for you). This is effectively analysis of deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562efaf2-d26f-46df-9194-22c1b2d7b680",
   "metadata": {},
   "source": [
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33265823-1b63-44b2-a6d6-b1988a8f9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans   In the Type I ANOVA order matters. So, whether you include iV1, or iV2 first makes a difference because the\n",
    "      first (e.g. iV1) is compared to a model with just an intercept while the second is compared to a model with \n",
    "    an intercept and the first. Try out switching the order they're in and you'll see the difference in main \n",
    "    effect outcomes because your predictors are correlated.\n",
    "    The Type III gets around this by assessing each predictor, including the interaction against a model including\n",
    "    everything but that predictor. That sounds like a good idea until you try to consider what an interaction is\n",
    "    without one of the main effects included. You're comparing the predictor to what is essentially a nonsensical\n",
    "    model (I think one of your references sort of goes into this). (Furthermore, my recollection is that Type III\n",
    "    is especially sensitive to missing cells (thus Type IV I believe).)\n",
    "    Type II gets around the order issue in Type I and compares sensible models (unlike Type III). Main effects are\n",
    "    tested with all other main effects in the model but not the interaction. Thus each main effect is easily \n",
    "    interpreted as the unique contribution of that predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2b329-3f35-4712-a771-5c0a1fd9b44b",
   "metadata": {},
   "source": [
    "10. Explain the concept of deviance in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331a6f8-8eab-4c13-869d-b2bf7dfe6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Deviance is a measure of goodness of fit of a generalized linear model1. It is a measure of badness of fit, \n",
    "     with higher numbers indicating worse fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6a23b-d9d7-4bbb-8f10-260b034a45a1",
   "metadata": {},
   "source": [
    "11. What is regression analysis and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e849f-e901-441c-9144-bea834651b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Regression analysis is a set of statistical methods used for the estimation of relationships between a \n",
    "     dependent variable and one or more independent variables,\n",
    "    The purpose of regression analysis is to describe the relationship between variables, and to estimate or \n",
    "    predict the value of one variable using the known values of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c16007-8bd3-4b66-8947-ea678aba7862",
   "metadata": {},
   "source": [
    "12. What is the difference between simple linear regression and multiple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2cdf2-32a8-42c6-a139-3495f0f49a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Simple linear regression has only one x and one y variable, while multiple linear regression has one y and two\n",
    "    or more x variables12. Simple linear regression is used when you have only one predictor, or X variable, \n",
    "    predicting the response or Y variable2. Multiple linear regression is used when you have multiple X predictors\n",
    "    that all contribute to predicting Y2. Simple linear regression occurs in 2 dimensions, while multiple linear \n",
    "    regression can occur in an infinite number of dimensions2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467481f-fb6b-4937-a97d-a2f565d4e5ea",
   "metadata": {},
   "source": [
    "13. How do you interpret the R-squared value in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d0a3a-d82c-41a2-9b9f-1885f428e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  R-squared is a measure of how well the regression model explains observed data12. It indicates the percentage\n",
    "    of variability in the target variable that is explained by the regression model. For example, an r-squared of\n",
    "    60% reveals that 60% of the variability observed in the target variable is explained by the regression model1.\n",
    "    Generally, a higher r-squared indicates more variability is explained by the model1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10667198-6606-4389-a1f0-108d58440dba",
   "metadata": {},
   "source": [
    "14. What is the difference between correlation and regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83486f99-ead8-4e92-a548-f49739ad91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The differences between correlation and regression are:\n",
    "    Correlation measures the strength and direction of a linear relationship between two variables, but does not\n",
    "    imply causation. Regression shows the effect of one variable on another and implies causation.\n",
    "    Correlation is based on a single statistic or data point, while regression is based on an equation and a line.\n",
    "    Correlation does not allow prediction of one variable from another, while regression does. Regression uses an \n",
    "    equation to quantify and predict the relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1773ca-8767-49d6-aaed-e32b9d988f79",
   "metadata": {},
   "source": [
    "15. What is the difference between the coefficients and the intercept in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0d0d9-8a29-4c9e-839e-9f2cccf5ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  In summary, the intercept is the value of y when all independent variables are zero, while the coefficients\n",
    "     measure the change in y for a unit change in each independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867873c-3a0b-454b-82b4-a3ed46d38b0e",
   "metadata": {},
   "source": [
    "16. How do you handle outliers in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fbcc8-264f-4f0c-99b2-eda940bfa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Criteria: data points that lie 1.5 times of IQR above Q3 and below Q1 are outliers. This shows in detail about\n",
    "    outlier treatment in Python.\n",
    "    steps:\n",
    "    Sort the dataset in ascending order\n",
    "    calculate the 1st and 3rd quartiles(Q1, Q3)\n",
    "    compute IQR=Q3-Q1\n",
    "    compute lower bound = (Q1–1.5*IQR), upper bound = (Q3+1.5*IQR)\n",
    "    loop through the values of the dataset and check for those who fall below the lower bound and above the upper\n",
    "    bound and mark them as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37ae3c-3a76-4cd8-86d6-dbb01ebb21eb",
   "metadata": {},
   "source": [
    "17. What is the difference between ridge regression and ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281aefb-dc90-4690-a12b-ed8542dc0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The difference between ordinary least squares and ridge regression is that ridge regression uses a biased\n",
    "    estimator that reduces the variance of the coefficients, while ordinary least squares uses an unbiased \n",
    "    estimator that minimizes the mean squared error12. Ridge regression also penalizes the magnitude of the \n",
    "    coefficients, which means that each feature has a smaller effect on the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb8577-6a5e-49f3-963e-df6ec90f14b9",
   "metadata": {},
   "source": [
    "18. What is heteroscedasticity in regression and how does it affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7f406-cd57-4afb-a753-20151fd6a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Heteroscedasticity refers to the unequal variance of the residuals in a regression analysis123. It violates\n",
    "     the assumption of the classical linear regression model and causes the regression coefficient estimates to be\n",
    "     unreliable and have higher variance13. This can lead to higher risk of type I error, which is rejecting a \n",
    "    true null hypothesis14. Heteroscedasticity can also affect the inclusion or exclusion of observations, \n",
    "    especially when the sample size is small3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bafb2-2a0b-4380-aa8a-fde9e697faed",
   "metadata": {},
   "source": [
    "19. How do you handle multicollinearity in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499887e7-e025-4d30-bb80-e4f18050380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  To deal with multicollinearity in a regression model, you can:\n",
    "    Remove some of the highly correlated independent variables.\n",
    "    Linearly combine the independent variables, such as adding them together.\n",
    "    Use partial least squares regression to create a set of uncorrelated components to include in the model.\n",
    "    Use LASSO and Ridge regression, which are advanced forms of regression analysis that can handle\n",
    "    multicollinearity.\n",
    "    Remove one or more variables showing a high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781adc07-a6a9-4936-a704-9bb298952b04",
   "metadata": {},
   "source": [
    "20. What is polynomial regression and when is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2831ae-e440-4636-97fa-8d323c76f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Polynomial Regression is used when the relationship between the dependent and independent variables is not \n",
    "     linear. It is generally used when the points in the data are not captured by the Linear Regression Model and \n",
    "    the Linear Regression fails in describing the best result clearly. Polynomial regression is used to fit a \n",
    "    regression model that describes the relationship between one or more predictor variables and a numeric \n",
    "    response variable. It involves fitting a polynomial function to the data points to obtain a curve that \n",
    "    represents the relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e57a1d-5e8a-4460-bdf9-546ed0c0be05",
   "metadata": {},
   "source": [
    "21. What is a loss function and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1fa1ef-3e11-47ff-ad5f-2932c2fca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The loss function is a method of evaluating how well a machine learning algorithm models a featured data\n",
    "    set. It serves as a gauge for how well the model can forecast the desired result1. The loss function \n",
    "    indicates how inaccurate the model is at determining the relationship between x and y. It is a metric that \n",
    "    the model utilizes to put a number to its performance, which is how close or far the model has made its \n",
    "    prediction to the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f966b2b-4508-4045-bac8-2feee8b6b606",
   "metadata": {},
   "source": [
    "22. What is the difference between a convex and non-convex loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b09fde-d594-4f64-9519-59118256c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  We know \" if a function is a non-convex loss function without plotting the graph \" by using Calculus. To \n",
    "     quote Wikipedia's convex function article: \" If the function is twice differentiable, and the second\n",
    "    derivative is always greater than or equal to zero for its entire domain, then the function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb88fa-9ed3-43d6-b119-05ea3147d6ce",
   "metadata": {},
   "source": [
    "23. What is mean squared error (MSE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3aa06-c2f7-469e-a030-30cf668c4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Mean Squared Error evaluates the proximity of a regression line to a group of data points. It is a risk \n",
    "    function that corresponds to the predicted squared error loss value. The mean square error is computed by \n",
    "    calculating the average, especially the mean, of the squared mistakes resulting from a function’s data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca829a9e-7f17-4ef5-9cc4-3198a293a790",
   "metadata": {},
   "source": [
    "24. What is mean absolute error (MAE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50316f65-57af-4aca-bd00-749f157ebd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Mean Absolute Error (MAE) is a way to measure the accuracy of a given model in statistics12. It is calculated\n",
    "     as: MAE = (1/n) * Σ|y i – x i | where: y i: The observed value for the i th observation x i: The predicted \n",
    "    value for the i th observation n: The total number of observations12. The formula for MAE is the average of \n",
    "    all absolute errors3. The steps to calculate MAE are easy: add all absolute errors, divide by the number of \n",
    "    errors3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae6c18-2609-452f-a86e-9c494b003dda",
   "metadata": {},
   "source": [
    "25. What is log loss (cross-entropy loss) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64718c7d-e9d5-4aa2-822f-6e219e431401",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Binary cross entropy (also known as logarithmic loss or log loss) is a model metric that tracks incorrect \n",
    "    labeling of the data class by a model, penalizing the model if deviations in probability occur into classifying\n",
    "    the labels. Low log loss values equate to high accuracy values. Binary cross entropy is equal to -1*log \n",
    "    (likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fca20-8ec3-42a8-b215-03577ff78ea6",
   "metadata": {},
   "source": [
    "26. How do you choose the appropriate loss function for a given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7446d-11e9-43cd-9fbb-fb217c932001",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Once you have considered these factors, you can choose a loss function that is appropriate for your problem.\n",
    "    Some common loss functions include mean squared error, cross-entropy, hinge loss, and log loss. It is also \n",
    "    important to evaluate the performance of your model using the chosen loss function and make adjustments as \n",
    "    necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef615b2-883f-47dd-815f-9a2ecfab1e09",
   "metadata": {},
   "source": [
    "27. Explain the concept of regularization in the context of loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e336de-74e1-4f2e-a110-f473a53e28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Regularization means restricting a model to avoid overfitting by shrinking the coefficient estimates to zero.\n",
    "    When a model suffers from overfitting, we should control the model's complexity. Technically, regularization\n",
    "    avoids overfitting by adding a penalty to the model's loss function: Regularization = Loss Function + Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e315a8-dd6a-4a9c-a248-a6c2d92560fe",
   "metadata": {},
   "source": [
    "28. What is Huber loss and how does it handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218b6e7-1b02-4778-a3ab-de2dc687fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Huber loss handles outliers by transitioning from a quadratic loss function (similar to mean squared error) \n",
    "    to a linear loss function (similar to mean absolute error) as the error increases. This transition is \n",
    "    controlled by the delta parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1e035-a694-4413-838f-310618edd958",
   "metadata": {},
   "source": [
    "29. What is quantile loss and when is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8984159-457b-44ef-87cf-a8c589bef321",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The quantile loss, also known as the pinball loss function, is a metric used to assess the accuracy of a \n",
    "    quantile forecast12. A quantile is the value below which a fraction of observations in a group falls2. The \n",
    "    quantile regression loss function is applied to predict quantiles2. It is an objective function that translates\n",
    "    the problem we are trying to solve into a mathematical formula to be minimized by the model2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770da0b-0572-4c38-a1ce-c17d6354ec2e",
   "metadata": {},
   "source": [
    "30. What is the difference between squared loss and absolute loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65e34-591a-4b91-a150-785983fd1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  If the prediction error causes the client's loss (e.g. financial loss) to grow quadratically and symmetrically\n",
    "     about zero, you are facing square prediction loss. If the client's loss grows linearly and symmetrically about\n",
    "    zero, you are facing absolute prediction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b000f-3db6-4157-a961-29981c81c12f",
   "metadata": {},
   "source": [
    "31. What is an optimizer and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01062b08-ef7d-4b21-9272-b36a5aff27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  In machine learning, an optimizer is an algorithm or method used to minimize an error function (loss function)\n",
    "    or to maximize the efficiency of production1. Optimizers are mathematical functions that are dependent on the\n",
    "    model’s learnable parameters, i.e., weights and biases1. The optimization process involves adjusting \n",
    "    hyperparameters to minimize the cost function using one of the optimization techniques2. The cost function \n",
    "    describes the discrepancy between the true value of the estimated parameter and what the model has predicted2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bd092-090b-4e48-8001-556f67af8146",
   "metadata": {},
   "source": [
    "32. What is Gradient Descent (GD) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83057d-7d9c-4413-a611-c9e1b63f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Gradient descent is an optimization algorithm used to train machine learning models and neural networks1. It \n",
    "    is used to find the values of a function's parameters that minimize a cost function as far as possible2. The \n",
    "    cost function acts as a barometer, gauging the accuracy of the model with each iteration of parameter updates.\n",
    "    Gradient descent uses calculus to iteratively adjust the values of the parameters so they minimize the given \n",
    "    cost-function2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f98d11-e598-485f-bc91-2e9e386a5b59",
   "metadata": {},
   "source": [
    "33. What are the different variations of Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393e37c-9c84-416e-838c-90c576863f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Types of gradient descent:\n",
    "     Batch gradient descent: updates the model parameters after calculating the error for each example in the \n",
    "    training dataset. It requires one epoch (pass through the entire dataset) per update.\n",
    "    Stochastic gradient descent: updates the model parameters after calculating the error for each training example\n",
    "    individually. It requires many epochs and is faster than batch gradient descent.\n",
    "    Mini-batch gradient descent: updates the model parameters after calculating the error for a subset of the \n",
    "    training dataset. It is a compromise between batch and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce95cf-f8a0-4ce8-8220-4b8de97d318a",
   "metadata": {},
   "source": [
    "34. What is the learning rate in GD and how do you choose an appropriate value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a0248-83e4-444d-b09d-f15508906b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The range of values to consider for the learning rate is less than 1.0 and greater than 10^-6. Typical values\n",
    "    for a neural network with standardized inputs (or inputs mapped to the (0,1) interval) are less than 1 and \n",
    "    greater than 10^−6 — Practical recommendations for gradient-based training of deep architectures, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab96eb5-c70a-4a4f-bcfe-f656c1b5e7b2",
   "metadata": {},
   "source": [
    "35. How does GD handle local optima in optimization problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e82e2-f800-47f3-bba1-8c34770c4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  So, stochastic gradient descent is more able to avoid local minimum because the landscape of batch loss \n",
    "    function is different than the loss function of whole dataset (the case when you calculate the losses on all \n",
    "    data and then update parameters). That means the gradient on the whole dataset could be 0 at some point, but \n",
    "    at that same point, the gradient of the batch could be different (so we hope to go in other direction than the\n",
    "    local minimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a96871-06c3-44f1-9fa9-b925a5864126",
   "metadata": {},
   "source": [
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0049df-66bc-4fd9-abe0-b3d8f02202ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The main difference between stochastic gradient descent (SGD) and gradient descent (GD) is the number of data\n",
    "    points used before each update of the parameters. GD spans over the entire dataset once before each update, \n",
    "    whereas SGD randomly takes just one data point for each update12. In GD, you have to run through all the \n",
    "    samples in your training set to do a single update for a parameter in a particular iteration, whereas in SGD,\n",
    "    you use only one or a subset of training samples from your training set to do the update for a parameter in a\n",
    "    particular iteration2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e688619-3ade-4b38-9860-bfc3efcb2b41",
   "metadata": {},
   "source": [
    "37. Explain the concept of batch size in GD and its impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99b984-448f-4814-b153-73163d1b4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Batch Size is among the important hyperparameters in Machine Learning. It is the hyperparameter that defines \n",
    "    the number of samples to work through before updating the internal model parameters. It can one of the crucial \n",
    "    steps to making sure your models hit peak performance.\n",
    "    The primary metric that we care about, Batch Size has an interesting relationship with model loss. Going with \n",
    "    the simplest approach, let’s compare the performance of models where the only thing that changes is the batch\n",
    "    size.The primary metric that we care about, Batch Size has an interesting relationship with model loss. Going\n",
    "    with the simplest approach, let’s compare the performance of models where the only thing that changes is the\n",
    "    batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f85f46-27f6-4ed6-b684-3959097964c2",
   "metadata": {},
   "source": [
    "38. What is the role of momentum in optimization algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac20ba-d340-443d-9b7a-310ad8c31727",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Momentum is an optimization technique that improves on gradient descent by reducing oscillatory effects and\n",
    "    acting as an accelerator for optimization problem solving12. It finds the global (and not just local) optimum,\n",
    "    and is commonly used in machine learning and has broad applications to all optimizers through SGD1. The basic \n",
    "    idea behind momentum is to decrease the convergence time by accelerating Gradient Descent in a relevant and \n",
    "    optimal direction3. It is widely used in deep learning applications and is an important optimization technique\n",
    "    for training deep neural networks2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73d393-c91f-42ed-87e8-09bf707e49d6",
   "metadata": {},
   "source": [
    "39. What is the difference between batch GD, mini-batch GD, and SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f310f-8022-4d3c-b1ab-69d1d64285a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Thus, the main difference between Batch, Mini-batch, and Stochastic Gradient Descent is the number of examples\n",
    "    used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ac033-6934-4564-a69a-c34411454108",
   "metadata": {},
   "source": [
    "40. How does the learning rate affect the convergence of GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb457a-3256-4bba-a8f0-1905e559775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  We will converge faster as we are not using whole data while updating the weight vector. And it will be the \n",
    "    close approximation to the minima as we are using a batch of training sample instead of a single point. As \n",
    "    batch size increases, the time it will take to reach minima (closer approximation) will increase, and the \n",
    "    accurate it will get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d378f-a77b-4c40-bd3f-4da58eaabf99",
   "metadata": {},
   "source": [
    "41. What is regularization and why is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5a300-875c-4846-8e9f-dd0dadc55f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Regularization is a technique in machine learning that is used to prevent overfitting or underfitting. It\n",
    "    involves adding a penalty term to the loss function during training, which encourages the model to choose \n",
    "    simpler solutions that generalize better to new data2. By using regularization, we can fit our machine \n",
    "    learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a5528-604c-4840-be16-6a12fab97513",
   "metadata": {},
   "source": [
    "42. What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0881cc-6b6e-4588-81eb-ec8cc05b124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The main differences between L1 and L2 regularization are:\n",
    "    L1 regularization generates sparse solutions and is helpful for feature selection, while L2 regularization \n",
    "    yields non-sparse solutions and is beneficial for building simpler models.\n",
    "    L1 regularization tries to estimate the median of the data, while L2 regularization tries to estimate the mean \n",
    "    of the data to avoid overfitting.\n",
    "    L1 regularization adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss \n",
    "    function, while L2 regularization adds the “squared magnitude” of the coefficient as the penalty term to the \n",
    "    loss function.\n",
    "    L2 regularization is the sum of the square of the weights, while L1 regularization is just the sum of the \n",
    "    weights.\n",
    "    L1 tends to shrink coefficients to zero, whereas L2 tends to shrink coefficients evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e67777-c25e-44b4-9704-b378a0870e9a",
   "metadata": {},
   "source": [
    "43. Explain the concept of ridge regression and its role in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd03cba-97c4-4c3f-a1e5-ea4c3bd89e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity.\n",
    "    This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased,\n",
    "    and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb28714-b9b9-4858-9fae-ed0590fb21b5",
   "metadata": {},
   "source": [
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c8a1d-0278-4673-85af-37c5e827e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Elastic net is a method that linearly combines L1 and L2 regularization with the goal to acquire the best of \n",
    "    both worlds123. Ridge utilizes an L2 penalty and lasso uses an L1 penalty. With elastic net, you don't have to \n",
    "    choose between these two models, because elastic net uses both the L2 and the L1 penalty1. The ElasticNet \n",
    "    mixing parameter determines the ratio of L1 and L2 penalties3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6c5df-000e-43b6-be5c-2ebaffee078d",
   "metadata": {},
   "source": [
    "45. How does regularization help prevent overfitting in machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc4fa6-0bb5-4180-9ca8-cc1eb8011175",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Regularization means restricting a model to avoid overfitting by shrinking the coefficient estimates to zero.\n",
    "    When a model suffers from overfitting, we should control the model's complexity. Technically, regularization\n",
    "    avoids overfitting by adding a penalty to the model's loss function: Regularization = Loss Function + Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311d9ba-9947-47c1-b54c-f286d4e6251b",
   "metadata": {},
   "source": [
    "46. What is early stopping and how does it relate to regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1166b-0376-409b-aa90-367becf9e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Early stopping can be thought of as implicit regularization, contrary to regularization via weight decay. This\n",
    "    method is also efficient since it requires less amount of training data, which is not always available. Due to\n",
    "    this fact, early stopping requires lesser time for training compared to other regularization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784bdd6-31c8-4b24-a8cf-5b94fced599d",
   "metadata": {},
   "source": [
    "47. Explain the concept of dropout regularization in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ba1fd-dd03-4ed6-9d5d-16f62ff61e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Dropout is a regularization method that approximates training a large number of neural networks with different\n",
    "    architectures in parallel.\n",
    "    During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of \n",
    "    making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to \n",
    "    the prior layer. In effect, each update to a layer during training is performed with a different “view” of the\n",
    "    configured layer.\n",
    "    Dropout regularization is a generic approach.\n",
    "    It can be used with most, perhaps all, types of neural network models, not least the most common network types\n",
    "    of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks.\n",
    "    In the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent \n",
    "    connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86225d99-abdd-4b59-a1c5-a3812955495c",
   "metadata": {},
   "source": [
    "48. How do you choose the regularization parameter in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651e3da-107f-416c-9f6b-b21da3fe310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Strategies to choose the regularization parameter include:\n",
    "    Estimating several different Ridge regressions with different values of the regularization parameter on the\n",
    "    training set, choosing the best model on the validation set, and checking for overfitting on the test set.\n",
    "    Using cross-validation or an information criterion, such as AIC or BIC.\n",
    "    Choosing a regularization method, such as Lasso regression or Ridge Regression, using a sequence of tuning\n",
    "    parameters to create a series of different models, and selecting the best model using methods such as Mallow’s\n",
    "    C p, AIC, or BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d7052-7bf6-4bdc-b476-6ebd0b59d9d5",
   "metadata": {},
   "source": [
    "49. What is the difference between feature selection and regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240babd3-672b-4b9f-9a5c-b0e2e6c9937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Feature selection, also known as feature subset selection, variable selection, or attribute selection. This\n",
    "    approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model\n",
    "    inference.Regularization, where we are constraining the solution space while doing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90871e87-86ed-4269-982a-12c7102c3c41",
   "metadata": {},
   "source": [
    "50. What is the trade-off between bias and variance in regularized models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3efe9b-781c-4448-82bb-4a591bba6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  There is a tradeoff between a model’s ability to minimize bias and variance which is referred to as the best \n",
    "    solution for selecting a value of Regularization constant. A proper understanding of these errors would help to\n",
    "    avoid the overfitting and underfitting of a data set while training the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dce4d-43d4-4a97-a26f-f177132915df",
   "metadata": {},
   "source": [
    "51. What is Support Vector Machines (SVM) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b084308-ee92-4df6-94b0-455e33ddca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Support vector machines (SVMs) are a set of supervised learning methods that can be used for classification, \n",
    "    regression and outliers detection123. SVMs construct a hyperplane or set of hyperplanes in a high or \n",
    "    infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers \n",
    "    detection2. SVMs are effective in high dimensional spaces and still effective in cases where the number of\n",
    "    dimensions is greater than the number of samples1. SVMs can be used for a variety of tasks, such as text \n",
    "    classification, image classification, spam detection, handwriting identification, gene expression analysis, \n",
    "    face detection, and anomaly detection4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81451ec2-c241-4326-825b-791127fbd2a4",
   "metadata": {},
   "source": [
    "52. How does the kernel trick work in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd14711-c648-4b08-b4f3-d7a99f284553",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The kernel trick is a technique that transforms low dimensional input space into a higher dimensional space,\n",
    "    where non-separable problems become separable12. The kernel trick allows the SVM algorithm to deal with \n",
    "    non-linear separation problems by using a kernel function that computes the similarity between pairs of points\n",
    "    in the higher dimensional feature space2. The kernel function does not need to explicitly calculate the \n",
    "    transformed feature representation, which saves computation time and memory2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2eede-bd59-4697-b347-350fbf2add9f",
   "metadata": {},
   "source": [
    "53. What are support vectors in SVM and why are they important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb570245-cf23-4c5f-b297-40d02cc27a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  In SVM, support vectors are the closest points of the lines from both classes12. These points are used to \n",
    "    calculate the distance between the vectors and the hyperplane, which is called the margin1. The SVM classifier\n",
    "    is a frontier that best segregates the two classes2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902939c1-e607-466c-b64f-2439bb75e75a",
   "metadata": {},
   "source": [
    "54. Explain the concept of the margin in SVM and its impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864336c6-e627-462e-9677-158254788461",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  One reasonable choice as the best hyperplane is the one that represents the largest separation or margin \n",
    "     between the two classes. So we choose the hyperplane whose distance from it to the nearest data point on each\n",
    "    side is maximized. If such a hyperplane exists it is known as the maximum-margin hyperplane/hard margin.\n",
    "    The SVM algorithm has the characteristics to ignore the outlier and finds the best hyperplane that maximizes\n",
    "    the margin. SVM is robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053bace-e756-4ecd-8a3f-f39917bee263",
   "metadata": {},
   "source": [
    "55. How do you handle unbalanced datasets in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd8437-02b3-4db5-a5b4-5218ba12efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans It's fine to have imbalanced data, because the SVM should be able to assign a greater penalty to\n",
    "    misclassification errors related with the less likely instance (e.g. \"True\" in your case), rather than assign \n",
    "    equal error weight which results in the undesirable classifier that assigns everything to the majority. However\n",
    "    ,you'll probably get better results with balanced data. It all depends on your data, really."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2664585-0ce3-487b-b946-4518301c0d88",
   "metadata": {},
   "source": [
    "56. What is the difference between linear SVM and non-linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6a576-025a-41f3-bf77-3bd570e70af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The main difference between a linear SVM and a non-linear SVM is that the linear SVM follows a simple rule,\n",
    "    where the dot product between two features of its input is equal to the linear combination of its input. \n",
    "    Linear SVM is used when data can be easily separated with a hyperplane by drawing a straight line. Non-linear\n",
    "    SVM, on the other hand, is used when data cannot be separated with a straight line, and kernel functions are\n",
    "    used instead. Non-linear SVMs generally achieve better performance, but linear SVMs are much faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f88d37-b5ca-4357-8534-8024301d5533",
   "metadata": {},
   "source": [
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6deea3-8be3-452e-932c-15daaae0374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  C: Margin maximisation and misclassification fines are balanced by the regularisation parameter C in SVM. The\n",
    "    penalty for going over the margin or misclassifying data items is decided by it. A stricter penalty is imposed\n",
    "    with a greater value of C, which results in a smaller margin and perhaps fewer misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b241204-a6ad-4ec8-b49a-3c2673f611f1",
   "metadata": {},
   "source": [
    "58. Explain the concept of slack variables in SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d06e3-d3e4-4da0-b1f6-6a8fa24e26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  In Support Vector Machines (SVM), slack variables are introduced to allow for some misclassification of \n",
    "    training data points. The slack variables are added to the optimization problem as a penalty term for \n",
    "    misclassification. The objective function is modified to minimize the sum of the slack variables and the \n",
    "    classification error. The slack variables are non-negative and represent the degree of misclassification of \n",
    "    each training point. The larger the value of the slack variable, the more misclassified the point is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d979b7-ef2e-4acf-8e93-5642f97ddebc",
   "metadata": {},
   "source": [
    "59. What is the difference between hard margin and soft margin in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65e3ef-1d06-4db8-b85c-d2f8c2e7e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  The difference between a hard margin and a soft margin in SVMs lies in the separability of the data. If our\n",
    "    data is linearly separable, we go for a hard margin. However, if this is not the case, it won’t be feasible to\n",
    "    do that. In such cases, we use a soft margin which allows some misclassifications in order to achieve a better\n",
    "    overall fit. Soft margin SVM could choose decision boundary that has non-zero training error even if dataset is\n",
    "    linearly separable, and is less likely to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792029b-1c2d-48ae-b943-89f82b296f9b",
   "metadata": {},
   "source": [
    "60. How do you interpret the coefficients in an SVM model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5246fe-2aaa-4322-a2cf-fd8ba1772ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Usually the coefficients of each independent variable tell us the importance of each feature. If you have a \n",
    "   very small (close to 0) coefficient associated to an independent variable it's probably due to the fact that the\n",
    "   model is not using that variable so much. It's not easy to interpret the weights, try to convince yourself that \n",
    "   weights define simply an hyperplane for separating data. Now try to think about the 2D case. If you have one of \n",
    "   the two coefficient that is close to zero you have a plane that is orthogonal to one of the two axes. So \n",
    "   probably your data can be separated by this type of hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774ea11-3d45-41b1-b30f-16c611756439",
   "metadata": {},
   "source": [
    "61. What is a decision tree and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6ab4e-6ccd-404e-a6dc-3df46bcd4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  A decision tree is a map of the possible outcomes of a series of related choices1. It is a decision support\n",
    "    hierarchical model that uses a tree-like model of decisions and their possible consequences, including chance \n",
    "    event outcomes, resource costs, and utility2. In its simplest form, a decision tree is a type of flowchart that\n",
    "    shows a clear pathway to a decision3. It is a powerful tool for modeling and predicting outcomes in a wide \n",
    "    range of domains, including business, finance, healthcare, and more4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673d059-e486-4a69-a708-35b9ad538674",
   "metadata": {},
   "source": [
    "62. How do you make splits in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7360cb7-acda-44d6-9a31-66f0965d4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Here are the steps to split a decision tree using the reduction in variance method:\n",
    "    For each split, individually calculate the variance of each child node\n",
    "    Calculate the variance of each split as the weighted average variance of child nodes\n",
    "    Select the split with the lowest variance\n",
    "    Perform steps 1-3 until completely homogeneous nodes are achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd9484-9a84-4e83-8965-128c636063ab",
   "metadata": {},
   "source": [
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06281eda-a381-45a8-a3b0-750693070675",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Gini impurity is a function that determines how well a decision tree was split. Basically, it helps us to\n",
    "    determine which splitter is best so that we can build a pure decision tree. Gini impurity ranges values from 0\n",
    "    to 0.5. It is one of the methods of selecting the best splitter; another famous method is Entropy which ranges\n",
    "    from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e447f45-4079-4df7-b03c-822af8dc34f5",
   "metadata": {},
   "source": [
    "64. Explain the concept of information gain in decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51262739-e8e9-4480-ba4f-e013284ad56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Information Gain: Information gain measures the reduction in entropy or variance that results from splitting a\n",
    "    dataset based on a specific property. It is used in decision tree algorithms to determine the usefulness of a\n",
    "    feature by partitioning the dataset into more homogeneous subsets with respect to the class labels or target\n",
    "    variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5f134-2c36-46ef-b179-d5beb071f959",
   "metadata": {},
   "source": [
    "65. How do you handle missing values in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859c5ba-0d36-4e54-a828-8bef73f8f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  There are different ways to handle missing values in decision trees. One way is to create a boolean or replace\n",
    "    the missing values by an \"outlier\" value1. If the amount of null values is quite insignificant, and your \n",
    "    dataset is large enough, you should consider deleting them1. Another way is to add a preprocessing step to \n",
    "    treat for missing values in the machine learning pipeline2. The CART algorithm makes it possible to handle \n",
    "    missing values within the Decision Tree itself2. The logic works by splitting the datasets into two subsets:\n",
    "    one containing missing values and the other one which doesn't3. The feature containing missing values might \n",
    "    need to be discretized, based upon the distribution diagram of the subset without missing values3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708048af-2308-45d3-9bba-f991346aa191",
   "metadata": {},
   "source": [
    "66. What is pruning in decision trees and why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5d5e7-cf99-46f6-a512-14ee6e160fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Pruning is a technique that reduces the size of decision trees by removing sections of the tree that are \n",
    "    non-critical and redundant1. Pruning helps to reduce the risk of overfitting and improve predictive accuracy.\n",
    "    Pruning requires a separate data set from the one used for growing the tree2. Pruning can be done by selecting\n",
    "    the nodes that you want to prune and clicking a button or using a menu option3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7985bd9-d595-4076-9a4e-1195b1ddd2fd",
   "metadata": {},
   "source": [
    "67. What is the difference between a classification tree and a regression tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe6aa1-7e73-40c1-98a9-342de399bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Classification trees are used to analyze discrete values14with dependent variables. Examples of classification\n",
    "    algorithms include decision tree and logistic regression1.Regression trees124are used to analyze continuous \n",
    "    real values14with dependent variables. Examples of regression algorithms include Random Forest and linear \n",
    "    regression1.The output variable in classification has to be a discrete value4, while the output variable in \n",
    "    regression has to be a continuous nature or real value4. Regression can be evaluated using root mean square \n",
    "    error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac80944-0f99-4eac-b9ff-614ce05c8e36",
   "metadata": {},
   "source": [
    "68. How do you interpret the decision boundaries in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2efdd2-a6e9-4e29-ba93-1a7c3601327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Decision boundaries are the line or surface that separates the decision space into two or more regions. Each\n",
    "     region is associated with a particular class label. When a new data point is presented, the model predicts the\n",
    "    class label of the point based on which region it falls into1. Decision tree does not learn to draw a decision\n",
    "    boundary. It tries to split the tree based on the maximum information gain point. Decision boundary of a \n",
    "    decision tree is determined by overlapping orthogonal half-planes (representing the result of each subsequence\n",
    "    decision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a820219-9b10-4cfa-a7f0-5e6607d99e17",
   "metadata": {},
   "source": [
    "69. What is the role of feature importance in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6734d-a087-41b0-b501-0c0067ded2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans  Feature importance in decision trees is a technique that assigns a score to features based on how significant\n",
    "    they are at predicting a target variable12. The scores are calculated on the weighted Gini indices1. The basic\n",
    "    idea for computing the feature importance for a specific feature involves computing the impurity metric of the\n",
    "    node subtracting the impurity metric of any child nodes3. Features that are involved in the top level nodes of\n",
    "    the decision trees tend to see more samples hence are likely to have more importance4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3cd5b-969d-425c-98e9-e2b03df71faf",
   "metadata": {},
   "source": [
    "70. What are ensemble techniques and how are they related to decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f4dc3-ed9e-427c-b335-dfa9cc91ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Ensemble techniques, such as bagging and boosting, offer a powerful approach to enhance the performance of \n",
    "    decision trees in machine learning tasks. By combining the predictions of multiple decision trees, these \n",
    "    techniques increase accuracy, improve robustness, and provide valuable insights into feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268d460-b008-41e5-8816-baf365812920",
   "metadata": {},
   "source": [
    "71. What are ensemble techniques in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b934a-6111-4f7e-892c-593bdee89aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Ensemble methods are techniques that create multiple models and then combine them to produce improved results.\n",
    "   They are used in machine learning to improve the accuracy of results in models by combining multiple models \n",
    "    instead of using a single model2. Ensemble methods usually produce more accurate solutions than a single model\n",
    "    would1. Ensemble learning methods are popular and the go-to technique when the best performance on a predictive\n",
    "    modeling project is the most important outcome3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ee6e9-35f7-44d1-9f5a-d7de97d22d90",
   "metadata": {},
   "source": [
    "72. What is bagging and how is it used in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d23a1-401d-45b3-9e31-afca1e6d08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Bagging is an ensemble machine learning algorithm that combines the predictions from many decision trees. It is\n",
    "    also known as bootstrap aggregation and is commonly used to reduce variance within a noisy dataset2. In bagging\n",
    "    , a random sample of data in a training set is selected with replacement, meaning that the individual data \n",
    "    points can be chosen more than once2. Bagging has few key hyperparameters and sensible heuristics for \n",
    "    configuring these hyperparameters, making it easy to implement1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20a30a-d72d-436e-a5bc-ac6927eeb86a",
   "metadata": {},
   "source": [
    "73. Explain the concept of bootstrapping in bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8b350-4f7a-402a-9771-970f500ad0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a\n",
    "    sample is chosen out of a set, using the replacement method. The learning algorithm is then run on the samples\n",
    "    selected. The bootstrapping technique uses sampling with replacements to make the selection procedure \n",
    "    completely random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113adcf-6a79-4d4e-b053-f48076d00bd2",
   "metadata": {},
   "source": [
    "74. What is boosting and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835e81b-e59b-4551-9f21-96d1f24a1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize\n",
    "    training errors. In boosting, a random sample of data is selected, fitted with a model and then trained \n",
    "    sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6976e7-b74f-4298-9fe2-d8c726f77030",
   "metadata": {},
   "source": [
    "75. What is the difference between AdaBoost and Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0c4c8-e4c7-4d20-ae58-6553ab7a7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans The main differences between AdaBoost and Gradient Boosting are:\n",
    "    Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while\n",
    "    AdaBoost can be seen as a special case with a particular loss function.\n",
    "    Gradient Boosting is more flexible than AdaBoost.\n",
    "    AdaBoost minimizes loss function related to any classification error and is best used with weak learners.\n",
    "    Weak learners are decision trees constructed in a greedy manner in Gradient Boosting, while in AdaBoost, the \n",
    "    weak learners are constructed in a sequential manner.\n",
    "    All the learners have equal weights in the Gradient Boosting algorithm, while in AdaBoost, the weights of the\n",
    "    learners are adjusted based on their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124be00-6e80-4c9c-a888-01fe60cd3a1f",
   "metadata": {},
   "source": [
    "76. What is the purpose of random forests in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368f1bf-ca8e-4382-9756-a62e5d71361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Random Forests are simply the combination of Decision Trees and Ensemble Learning. Rather than using a single\n",
    "    tree, that is inherently easily over trained, you use a whole forest of trees that are trained to be unique in\n",
    "    their approach. This prevents the over training of an individual tree from effecting how the model generalizes\n",
    "    to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d361ee-db27-4a29-8eeb-f23a511a5beb",
   "metadata": {},
   "source": [
    "77. How do random forests handle feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255b37b-20f8-4675-b1a3-65eba541d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans In Random Forest, feature importance is computed based on out-of-bag (OOB) error. The random forest model is\n",
    "    created and then the OOB error is computed. This is followed by permuting (shuffling) a feature and then again\n",
    "    the OOB error is computed. Like wise, all features are permuted one by one1. Feature importance in random \n",
    "    forest is usually calculated in two ways: impurity importance (mean decrease impurity) and permutation \n",
    "    importance (mean decrease accuracy). The impurity importance of each variable is the sum of impurity decrease\n",
    "    of all trees when it is selected to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ff87c-325b-42c4-8ca7-7b4ba343cc67",
   "metadata": {},
   "source": [
    "78. What is stacking in ensemble learning and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662825cb-f4f8-4ddd-9838-53bcc7d4c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Stacking is an ensemble machine learning technique that involves combining the predictions of multiple models\n",
    "    to build a new model with improved performance123. It is also known as stacked generalization23. Stacking \n",
    "    enables us to train multiple models to solve similar problems, and based on their combined output, it builds a\n",
    "    new model with improved performance1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496b986-0ecb-4d49-bede-1ad5bc76a93c",
   "metadata": {},
   "source": [
    "79. What are the advantages and disadvantages of ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb253c52-311f-4d18-a3c4-920ba5010024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans Ensemble techniques can improve the average prediction performance over any contributing member in the \n",
    "    ensemble. The mechanism for improved performance with ensembles is often the reduction in the variance \n",
    "    component of prediction errors made by the contributing models. Ensemble methods include bagging, boosting,\n",
    "    and stacking. Bagging reduces variance and improves accuracy, but can increase bias and may not work well with\n",
    "    noisy data. Boosting improves accuracy and reduces bias, but can overfit with noisy data and outliers. \n",
    "    Stacking improves prediction accuracy by combining models, but can be complex and time-consuming to implement.\n",
    "    Ensemble learning can help project managers to deal with both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e3b9b-e136-4711-b3b8-6b99b88a8b4c",
   "metadata": {},
   "source": [
    "80. How do you choose the optimal number of models in an ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d27a7b-ecdd-40f5-a85f-468bc72ca2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep \n",
    "    the number of models as a hyperparameter if the training cost is less. Typically, you will observe the slanted\n",
    "    'L' shaped curve for MSE vs # of models plot. You can take the elbow point as the final # of models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
